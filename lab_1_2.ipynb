{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9bpdDoNhvj3Vx+38CccfK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaya-02ly/NLP_/blob/main/lab_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHbSdjSNDbxK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ab4433e"
      },
      "source": [
        "# Task\n",
        "Perform text preprocessing on a user-provided paragraph by installing and importing necessary libraries (NLTK, spaCy), loading the text, counting the total number of words, converting the text to lowercase, and finally summarizing the performed preprocessing steps, including the word count and the lowercased text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63d77ae3"
      },
      "source": [
        "## Install and Import Libraries\n",
        "\n",
        "### Subtask:\n",
        "Install the NLTK and spaCy libraries. Then, import these libraries along with any other necessary modules for text processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40c4cf66"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I need to install the `nltk` library as per the instructions. This requires a separate code block for the `!pip install` command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe23a4de",
        "outputId": "226353fe-6f82-4742-9617-a149cfa32beb"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b866492"
      },
      "source": [
        "**Reasoning**:\n",
        "Next, I need to install the `spaCy` library as per the instructions. This requires a separate code block for the `!pip install` command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60af2850",
        "outputId": "cd779b18-4dd0-4a1a-d0d8-41a591bdc991"
      },
      "source": [
        "import sys\n",
        "!pip install spacy\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26920d6b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that both NLTK and spaCy are installed, I will import them as instructed. Additionally, I need to download the 'punkt' tokenizer data for NLTK, which is a necessary step for many NLTK text processing functionalities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04505ba9",
        "outputId": "793fd0d9-c938-4880-b96f-bda70749c8ed"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"NLTK and spaCy imported, and NLTK 'punkt' data downloaded.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK and spaCy imported, and NLTK 'punkt' data downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0814f133"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps successfully installed and imported the necessary libraries (NLTK and spaCy) and downloaded the 'punkt' tokenizer data. The next logical step, as per the task description, is to load the user-provided text for preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38909e3e",
        "outputId": "f9f5eb23-6e12-4fa1-fce9-59577a87b792"
      },
      "source": [
        "user_paragraph = \"This is an example paragraph that will be used for text preprocessing. It contains several sentences and needs to be cleaned and analyzed.\"\n",
        "print(\"User paragraph loaded:\")\n",
        "print(user_paragraph)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User paragraph loaded:\n",
            "This is an example paragraph that will be used for text preprocessing. It contains several sentences and needs to be cleaned and analyzed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5974aad7"
      },
      "source": [
        "## Count Words\n",
        "\n",
        "### Subtask:\n",
        "Count the total number of words in the loaded text using appropriate Python string methods or NLTK functionalities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d71b732"
      },
      "source": [
        "**Reasoning**:\n",
        "As per the instructions, I will use `nltk.word_tokenize()` to break down the `user_paragraph` into individual words and then use `len()` to get the total word count. Finally, I will print the `word_count`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87230fba",
        "outputId": "85df0e01-ecf2-41e9-d3e9-61f095e87104"
      },
      "source": [
        "words = nltk.word_tokenize(user_paragraph)\n",
        "word_count = len(words)\n",
        "\n",
        "print(f\"Total word count: {word_count}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word count: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "163e7e1b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because NLTK's `punkt_tab` resource was not found. To fix this, I need to download the `punkt_tab` tokenizer data, which is explicitly suggested by the error message and is a necessary prerequisite for `nltk.word_tokenize()` to function correctly in this context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a98d618",
        "outputId": "84317367-9fab-4d14-8b71-7e4e8888d949"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "print(\"NLTK 'punkt_tab' data downloaded, which is required for tokenization.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK 'punkt_tab' data downloaded, which is required for tokenization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28de1f5d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error indicated that the `punkt_tab` resource was missing, which has now been downloaded. I will re-run the word counting code, as the prerequisite for `nltk.word_tokenize()` is now met.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eb895a9",
        "outputId": "5b55cebc-54db-422b-80fc-e789dc1e40e2"
      },
      "source": [
        "words = nltk.word_tokenize(user_paragraph)\n",
        "word_count = len(words)\n",
        "\n",
        "print(f\"Total word count: {word_count}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word count: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98c39d44"
      },
      "source": [
        "## Convert to Lowercase\n",
        "\n",
        "### Subtask:\n",
        "Convert the entire text to lowercase using Python's built-in string methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d26715f"
      },
      "source": [
        "**Reasoning**:\n",
        "As per the instructions, I need to convert the `user_paragraph` to lowercase using the `.lower()` method, store it in `lower_case_paragraph`, and then print the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c9b3937",
        "outputId": "b109d9ce-691f-4c07-c03e-aff2475fdcbf"
      },
      "source": [
        "lower_case_paragraph = user_paragraph.lower()\n",
        "\n",
        "print(\"Lowercased paragraph:\")\n",
        "print(lower_case_paragraph)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercased paragraph:\n",
            "this is an example paragraph that will be used for text preprocessing. it contains several sentences and needs to be cleaned and analyzed.\n"
          ]
        }
      ]
    }
  ]
}